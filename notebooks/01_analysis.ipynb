{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "week1=pd.read_csv(\"nfl-big-data-bowl-2024/tracking_week_1.csv\")\n",
    "week2=pd.read_csv(\"nfl-big-data-bowl-2024/tracking_week_2.csv\")\n",
    "week3=pd.read_csv(\"nfl-big-data-bowl-2024/tracking_week_3.csv\")\n",
    "week4=pd.read_csv(\"nfl-big-data-bowl-2024/tracking_week_4.csv\")\n",
    "week5=pd.read_csv(\"nfl-big-data-bowl-2024/tracking_week_5.csv\")\n",
    "week6=pd.read_csv(\"nfl-big-data-bowl-2024/tracking_week_6.csv\")\n",
    "week7=pd.read_csv(\"nfl-big-data-bowl-2024/tracking_week_7.csv\")\n",
    "week8=pd.read_csv(\"nfl-big-data-bowl-2024/tracking_week_8.csv\")\n",
    "week9=pd.read_csv(\"nfl-big-data-bowl-2024/tracking_week_9.csv\")\n",
    "plays=pd.read_csv(\"nfl-big-data-bowl-2024/plays.csv\")\n",
    "games=pd.read_csv(\"nfl-big-data-bowl-2024/games.csv\")\n",
    "players=pd.read_csv(\"nfl-big-data-bowl-2024/players.csv\")\n",
    "tackles=pd.read_csv(\"nfl-big-data-bowl-2024/tackles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plays=pd.concat([week1, week2, week3, week4, week5, week6, week7, week8, week9],axis=0)\n",
    "plays[\"uniqueid\"]=plays[\"gameId\"]*plays[\"playId\"]\n",
    "all_plays[\"uniqueid\"]=all_plays[\"gameId\"]*all_plays[\"playId\"]\n",
    "plays_final=pd.merge(plays,games, on=\"gameId\",how=\"right\")\n",
    "plays_final=plays_final[plays_final[\"playNullifiedByPenalty\"]==\"N\"]\n",
    "plays_final.drop([\"gameId\",\"playId\",\"ballCarrierDisplayName\",\"playDescription\",\"prePenaltyPlayResult\",\"playNullifiedByPenalty\",\n",
    "                 \"foulName1\",\"foulName2\",\"penaltyYards\",\"foulNFLId1\",\"foulNFLId2\",\"season\",\"gameDate\",\"gameTimeEastern\"],axis=1,inplace=True)\n",
    "def assign_offense_defense(row):\n",
    "    if row['possessionTeam'] == row['homeTeamAbbr']:\n",
    "        row['offenseScore'] = row['preSnapHomeScore']\n",
    "        row['defenseScore'] = row['preSnapVisitorScore']\n",
    "        row['offenseWinProbability'] = row['preSnapHomeTeamWinProbability']\n",
    "        row['offenseWPAdded']=row[\"homeTeamWinProbabilityAdded\"]\n",
    "        row['defenseWinProbability'] = row['preSnapVisitorTeamWinProbability']\n",
    "        row['defenseWPAdded']=row[\"visitorTeamWinProbilityAdded\"]\n",
    "    else:\n",
    "        row['offenseScore'] = row['preSnapVisitorScore']\n",
    "        row['defenseScore'] = row['preSnapHomeScore']\n",
    "        row['offenseWinProbability'] = row['preSnapVisitorTeamWinProbability']\n",
    "        row['offenseWPAdded']=row[\"visitorTeamWinProbilityAdded\"]\n",
    "        row['defenseWinProbability'] = row['preSnapHomeTeamWinProbability']\n",
    "        row['defenseWPAdded']=row[\"homeTeamWinProbabilityAdded\"]\n",
    "    return row\n",
    "\n",
    "plays_final = plays_final.apply(assign_offense_defense, axis=1)\n",
    "plays_final=plays_final.drop([\"preSnapHomeScore\",\"preSnapVisitorScore\",\"preSnapHomeTeamWinProbability\",\"preSnapVisitorTeamWinProbability\",\n",
    "                             \"homeFinalScore\",\"visitorFinalScore\",\"homeTeamWinProbabilityAdded\",\"visitorTeamWinProbilityAdded\"], axis=1)\n",
    "all_plays=pd.merge(all_plays, games, on=\"gameId\",how=\"right\")\n",
    "all_plays.drop([\"gameDate\",\"gameTimeEastern\",\"homeTeamAbbr\",\"visitorTeamAbbr\",\"homeFinalScore\",\"visitorFinalScore\"],axis=1, inplace=True)\n",
    "all_id=plays_final.uniqueid\n",
    "final_data=pd.merge(all_plays, plays_final, on=\"uniqueid\",how=\"right\")\n",
    "final_data.drop([\"gameId\",\"playId\",\"time\",\"jerseyNumber\",\"season\",\"yardlineSide\",\n",
    "                     \"yardlineNumber\",\"passResult\",\"passLength\",\"week_y\",\"homeTeamAbbr\",\"visitorTeamAbbr\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = ['x', 'y', 's', 'a', 'dis']\n",
    "rows_with_all_nas = final_data[columns_to_check].isna().all(axis=1)\n",
    "final_data = final_data[~rows_with_all_nas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "safefumble_ids = final_data[(final_data['event'] == 'fumble') | (final_data['event'] == 'safety')]['uniqueid'].unique()\n",
    "nfinal_data = final_data[~final_data['uniqueid'].isin(safefumble_ids)]\n",
    "for index, row in nfinal_data.iterrows():\n",
    "    if row['playDirection']==\"left\":\n",
    "        nfinal_data.loc[index,\"x\"]=120-row[\"x\"]\n",
    "        nfinal_data.loc[index,\"o\"]=360-row[\"o\"]\n",
    "        nfinal_data.loc[index,\"dir\"]=360-row[\"dir\"]\n",
    "        nfinal_data.loc[index,\"absoluteYardlineNumber\"]=120-row[\"absoluteYardlineNumber\"]\n",
    "nfinal_data = nfinal_data.copy()\n",
    "\n",
    "# optional: ensure playResult is numeric\n",
    "nfinal_data[\"playResult\"] = pd.to_numeric(nfinal_data[\"playResult\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# assign using .loc (all rows)\n",
    "nfinal_data.loc[:, \"AbsolutePostPlay\"] = (\n",
    "    nfinal_data[\"absoluteYardlineNumber\"] + nfinal_data[\"playResult\"]\n",
    ")\n",
    "nfinal_data=nfinal_data[nfinal_data[\"uniqueid\"]!=4753982039408]\n",
    "nfinal_data=nfinal_data[nfinal_data[\"uniqueid\"]!=3912749001045]\n",
    "nfinal_data=nfinal_data[nfinal_data[\"uniqueid\"]!=4147333259153]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_groups = nfinal_data.groupby('uniqueid').filter(lambda x: ~((x['event'] == 'handoff') | (x['event'] == 'pass_outcome_caught') | (x[\"event\"]==\"run\")).any())\n",
    "unique_ids_without_events = filtered_groups['uniqueid'].unique()\n",
    "nfinal_data=nfinal_data[~nfinal_data['uniqueid'].isin(unique_ids_without_events)]\n",
    "relevant_events = nfinal_data[nfinal_data['event'].isin(['handoff', 'run', 'pass_outcome_caught'])]\n",
    "event_counts = relevant_events.groupby('uniqueid')['event'].nunique()\n",
    "plays_with_multiple_events = event_counts[event_counts > 1].index.tolist()\n",
    "nfinal_data=nfinal_data[~nfinal_data['uniqueid'].isin(plays_with_multiple_events)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_frame_ids = nfinal_data[nfinal_data['event'].isin(['handoff', 'run', 'pass_outcome_caught'])].groupby('uniqueid')['frameId'].min()\n",
    "nfinal_data = nfinal_data.merge(ref_frame_ids.rename('ref_frameId'), on='uniqueid', how='left')\n",
    "nfinal_data = nfinal_data[nfinal_data['frameId'] >= nfinal_data['ref_frameId']]\n",
    "nfinal_data.drop([\"ref_frameId\"],axis=1, inplace=True)\n",
    "def calculate_distance(x1, y1, x2, y2):\n",
    "    return sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "nfinal_data['tackleOpportunity'] = False\n",
    "row_counter=0\n",
    "for unique_id in nfinal_data['uniqueid'].unique():\n",
    "    play_data = nfinal_data[nfinal_data['uniqueid'] == unique_id]\n",
    "    possession_team = play_data['possessionTeam'].iloc[0]\n",
    "    defensive_team = play_data['defensiveTeam'].iloc[0]\n",
    "    for frame_id in play_data['frameId'].unique():\n",
    "        frame_data = play_data[play_data['frameId'] == frame_id]\n",
    "        ball_carrier_row = frame_data[frame_data['nflId'] == frame_data['ballCarrierId'].iloc[0]]\n",
    "        if not ball_carrier_row.empty:\n",
    "            ball_carrier_x = ball_carrier_row['x'].iloc[0] \n",
    "            ball_carrier_y = ball_carrier_row['y'].iloc[0]\n",
    "            defenders = frame_data[frame_data['club'] == defensive_team]\n",
    "            for _, defender in defenders.iterrows():\n",
    "                row_counter += 1  \n",
    "                if(row_counter % 10000==0):\n",
    "                    print(f\"Processing row number: {row_counter}\")\n",
    "                distance = calculate_distance(ball_carrier_x, ball_carrier_y, defender['x'], defender['y'])\n",
    "                if distance <= 2.5:\n",
    "                    nfinal_data.loc[(nfinal_data['uniqueid'] == unique_id) & (nfinal_data['frameId'] == frame_id) &\n",
    "                                       (nfinal_data['nflId']== defender['nflId']), 'tackleOpportunity'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfinal_data['tackleOpportunity'] = nfinal_data['tackleOpportunity'].astype(int)\n",
    "defenders_df = nfinal_data[nfinal_data['club'] == nfinal_data['defensiveTeam']]\n",
    "defender_tackle_opportunities = defenders_df.groupby(['frameId', 'uniqueid']).agg({'tackleOpportunity': 'sum'}).rename(columns={'tackleOpportunity': 'defenderTackleOpportunities'})\n",
    "new_cmp = nfinal_data.merge(defender_tackle_opportunities, on=['frameId', 'uniqueid'], how='left')\n",
    "solo_tackle=new_cmp[new_cmp[\"defenderTackleOpportunities\"]==1]\n",
    "def identify_tackler(sub_df):\n",
    "    tackler = sub_df[sub_df['tackleOpportunity'] == 1]['nflId'].iloc[0]\n",
    "    return pd.Series({'tacklerNflId': tackler})\n",
    "tackler_df = solo_tackle.groupby(['uniqueid', 'frameId']).apply(identify_tackler).reset_index()\n",
    "solo_tackle = pd.merge(solo_tackle, tackler_df, on=['uniqueid', 'frameId'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tackles[\"uniqueid\"]=tackles[\"gameId\"]*tackles[\"playId\"]\n",
    "tackles=tackles[(tackles[\"tackle\"]==1) | (tackles[\"assist\"]==1)]\n",
    "index_to_drop = tackles[tackles['uniqueid'] == 3799509190053].index\n",
    "tackles.drop(index_to_drop, inplace=True)\n",
    "combos=tackles[tackles[\"assist\"]==1][[\"uniqueid\",\"nflId\"]].drop_duplicates()\n",
    "tackles=tackles[tackles[\"tackle\"]==1]\n",
    "tackles.drop([\"tackle\",\"gameId\",\"playId\",\"assist\",\"forcedFumble\",\"pff_missedTackle\"],axis=1, inplace=True)\n",
    "solo_tackle = pd.merge(solo_tackle, tackles, on='uniqueid', how='left')\n",
    "solo_tackle['nflId_y'].fillna(0, inplace=True)\n",
    "solo_tackle['tackleMade'] = (solo_tackle['nflId_y'] == solo_tackle['tacklerNflId']).astype(int)\n",
    "solo_tackle.drop([\"nflId_y\"],axis=1,inplace=True)\n",
    "solo_tackle=solo_tackle.sort_values(by=[\"uniqueid\",\"frameId\"])\n",
    "solo_tackle.drop([\"defenderTackleOpportunities\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = solo_tackle\n",
    "ntrial = trial.loc[trial[\"tackleOpportunity\"] == 1].copy()   # <- make an explicit copy\n",
    "ntrial.reset_index(drop=True, inplace=True)\n",
    "ntrial.loc[:, 'startTackleOpportunity'] = 0                  # <- use .loc for assignment\n",
    "ntrial.at[0, 'startTackleOpportunity'] = 1\n",
    "for i in range(1, len(ntrial)):\n",
    "    if (ntrial.at[i, 'nflId_x'] == ntrial.at[i - 1, 'nflId_x']) and \\\n",
    "       (ntrial.at[i, 'uniqueid'] == ntrial.at[i - 1, 'uniqueid']) and \\\n",
    "       (ntrial.at[i, 'frameId'] == ntrial.at[i - 1, 'frameId'] + 1):\n",
    "        ntrial.at[i, 'startTackleOpportunity'] = 0\n",
    "    else:\n",
    "        ntrial.at[i, 'startTackleOpportunity'] = 1\n",
    "\n",
    "ntrial_filtered = ntrial[ntrial['startTackleOpportunity'] == 1]\n",
    "indices = ntrial_filtered.groupby(['nflId_x', 'uniqueid'])['frameId'].idxmax()\n",
    "ntrial_filtered = ntrial_filtered.loc[indices]\n",
    "legal = ntrial_filtered[[\"frameId\", \"uniqueid\"]]\n",
    "solo_tackle_final = solo_tackle.merge(legal, on=['frameId', 'uniqueid'], how='inner')\n",
    "merged_data = solo_tackle.merge(ntrial_filtered, on=['frameId', 'uniqueid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "solo_tackle_final=solo_tackle_final[solo_tackle_final[\"displayName\"]!=\"football\"]\n",
    "solo_tackle_final[['x', 'y']] = solo_tackle_final[['x', 'y']].apply(pd.to_numeric, errors='coerce')\n",
    "filtered_df = pd.DataFrame()\n",
    "grouped = solo_tackle_final.groupby(['frameId', 'uniqueid'])\n",
    "for (frame, unique_id), group in grouped:\n",
    "    group = group.copy()\n",
    "    ball_carrier = group[group['nflId_x'] == group['ballCarrierId']]\n",
    "    tackler = group[group['tackleOpportunity'] == 1]\n",
    "    if not ball_carrier.empty:\n",
    "        ball_carrier_x, ball_carrier_y = ball_carrier.iloc[0][['x', 'y']]\n",
    "        group['distance_to_ball_carrier'] = ((group['x'] - ball_carrier_x) ** 2 + (group['y'] - ball_carrier_y) ** 2) ** 0.5\n",
    "        distances = group[~group['nflId_x'].isin(ball_carrier['nflId_x']) & ~group['nflId_x'].isin(tackler['nflId_x'])]\n",
    "        closest_players = distances.nsmallest(3, 'distance_to_ball_carrier')\n",
    "        frame_filtered = pd.concat([ball_carrier, tackler, closest_players])\n",
    "        filtered_df = pd.concat([filtered_df, frame_filtered])\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "data_final=filtered_df\n",
    "data_final.drop([\"playDirection\",\"defenseWinProbability\",\"tackleOpportunity\"],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "data_final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_seconds(time_str):\n",
    "    if pd.isna(time_str): \n",
    "        return None\n",
    "    minutes, seconds = map(int, time_str.split(':'))\n",
    "    return minutes * 60 + seconds\n",
    "data_final['gameClock'] = data_final['gameClock'].apply(convert_to_seconds)\n",
    "data_final = data_final.merge(players[['nflId', 'height', 'weight']], left_on='ballCarrierId', right_on='nflId', how='left')\n",
    "data_final.rename(columns={'height': 'ballCarrierHeight', 'weight': 'ballCarrierWeight'}, inplace=True)\n",
    "data_final.drop(columns='nflId', inplace=True)\n",
    "data_final = data_final.merge(players[['nflId', 'height', 'weight']], left_on='tacklerNflId', right_on='nflId', how='left')\n",
    "data_final.rename(columns={'height': 'tacklerHeight', 'weight': 'tacklerWeight'}, inplace=True)\n",
    "data_final.drop(columns='nflId', inplace=True)\n",
    "def convert_height_to_inches(height):\n",
    "    feet, inches = height.split('-')\n",
    "    return int(feet) * 12 + int(inches)\n",
    "data_final['tacklerHeight'] = data_final['tacklerHeight'].apply(convert_height_to_inches)\n",
    "data_final['ballCarrierHeight'] = data_final['ballCarrierHeight'].apply(convert_height_to_inches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final['side'] = data_final.apply(lambda row: 'offense' if row['club'] == row['possessionTeam'] else 'defense', axis=1)\n",
    "data_final.drop([\"displayName\",\"nflId_x\",\"club\",\"event\",\"possessionTeam\",\"defensiveTeam\",\"distance_to_ball_carrier\"],axis=1, inplace=True)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder=LabelEncoder()\n",
    "data_final[\"offenseFormation\"]=labelencoder.fit_transform(data_final[\"offenseFormation\"])\n",
    "data_final[\"side\"]=labelencoder.fit_transform(data_final[\"side\"])\n",
    "data_final['idx'] = data_final.groupby(['uniqueid','frameId']).cumcount() + 1\n",
    "pivot_df = data_final.pivot(index=['uniqueid','frameId','week_x','ballCarrierId','absoluteYardlineNumber','AbsolutePostPlay',\n",
    "                                   'quarter','down','yardsToGo',\n",
    "                                   'offenseScore','defenseScore','defendersInTheBox','offenseFormation',\"expectedPoints\",\n",
    "                                   'passProbability','offenseWinProbability','gameClock',\n",
    "                                  'tacklerHeight','tacklerWeight','ballCarrierHeight','ballCarrierWeight',\n",
    "                                  'tacklerNflId','tackleMade'], columns='idx')\n",
    "pivot_df.columns = [f'{col[0]}{col[1]}' for col in pivot_df.columns]\n",
    "pivot_df.reset_index(inplace=True)\n",
    "combos.rename(columns={\"nflId\":\"tacklerNflId\",\"uniqueid\":\"uniqueid\"},inplace=True)\n",
    "merged=pd.merge(pivot_df, combos, how=\"left\",indicator=True,on=[\"tacklerNflId\",\"uniqueid\"])\n",
    "pivot_dfnew=merged[merged[\"_merge\"]==\"left_only\"]\n",
    "pivot_dfnew.drop(columns=[\"_merge\"],inplace=True)\n",
    "\n",
    "train=pivot_dfnew[pivot_dfnew[\"week_x\"]>=5]\n",
    "test=pivot_dfnew[pivot_dfnew[\"week_x\"]<5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_count = train.isna().sum()\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(na_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = train[\"uniqueid\"].values\n",
    "train_frame = train[\"frameId\"].values\n",
    "train_carrier = train[\"ballCarrierId\"].values\n",
    "train_tackler = train[\"tacklerNflId\"].values\n",
    "\n",
    "test_id = test[\"uniqueid\"].values\n",
    "test_frame = test[\"frameId\"].values\n",
    "test_carrier = test[\"ballCarrierId\"].values\n",
    "test_tackler = test[\"tacklerNflId\"].values\n",
    "\n",
    "train = train.copy()\n",
    "test  = test.copy()\n",
    "\n",
    "train.drop([\"uniqueid\",\"frameId\",\"ballCarrierId\",\"tacklerNflId\",\"AbsolutePostPlay\"], axis=1, inplace=True)\n",
    "test.drop([\"uniqueid\",\"frameId\",\"ballCarrierId\",\"tacklerNflId\",\"AbsolutePostPlay\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train[\"tackleMade\"].values\n",
    "X = train.drop([\"tackleMade\"], axis=1).values\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "# FIX: use scipy.stats distributions (not numpy/random scalars)\n",
    "from scipy.stats import randint as sp_randint, uniform as sp_uniform\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic', \n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators':      sp_randint(50, 1001),   # integers [50, 1000]\n",
    "    'learning_rate':     sp_uniform(0.01, 0.59), # floats in [0.01, 0.60)\n",
    "    'max_depth':         sp_randint(3, 11),      # integers [3, 10]\n",
    "    'min_child_weight':  sp_randint(1, 11),      # integers [1, 10]\n",
    "    'subsample':         sp_uniform(0.5, 0.5),   # [0.5, 1.0)\n",
    "    'colsample_bytree':  sp_uniform(0.5, 0.5),   # [0.5, 1.0)\n",
    "    'gamma':             sp_uniform(0.1, 0.6),   # [0.1, 0.7)\n",
    "    'reg_alpha':         sp_randint(1, 8)        # integers [1, 7]\n",
    "}\n",
    "\n",
    "random_search_xgb = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=400,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    random_state=39,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    error_score=\"raise\"\n",
    ")\n",
    "random_search_xgb.fit(X, y)\n",
    "# Print the best score\n",
    "print(\"Best Score:\", random_search_xgb.best_score_)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", random_search_xgb.best_params_)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=965,\n",
    "    learning_rate=0.021,\n",
    "    max_depth=9,\n",
    "    min_child_weight=2,\n",
    "    subsample=0.869,\n",
    "    colsample_bytree=0.909,\n",
    "    gamma=0.239,\n",
    "    reg_alpha=1,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "%time scores = cross_val_score(xgb_model, X, y, cv=kf, scoring='roc_auc')\n",
    "for i, score in enumerate(scores):\n",
    "    print(f'Fold {i+1}:  = {score:.2f}')\n",
    "avg_score = np.mean(scores)\n",
    "print(f'Average SCORE: {avg_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~np.isnan(X).any(axis=1)\n",
    "\n",
    "X = X[mask]\n",
    "y = y[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 1001),\n",
    "    'max_depth': randint(10, 100),\n",
    "    'min_samples_split': randint(2, 11),\n",
    "    'min_samples_leaf': randint(1, 11),\n",
    "    'bootstrap': [True, False],\n",
    "}\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    estimator=rf_model, \n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50, \n",
    "    scoring='roc_auc',\n",
    "    cv=5, \n",
    "    random_state=30, \n",
    "    n_jobs=-1, \n",
    "    verbose=1\n",
    ")\n",
    "%time random_search_rf.fit(X,y)\n",
    "# Print the best score\n",
    "print(\"Best Score:\", random_search_rf.best_score_)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", random_search_rf.best_params_)\n",
    "\n",
    "np.random.seed(42)\n",
    "kf=KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rf_model = RandomForestClassifier(bootstrap=False, min_samples_leaf=4, min_samples_split=9, n_estimators=937,random_state=42, max_depth=27,\n",
    "                                 )\n",
    "%time scores=cross_val_score(rf_model, X, y, cv=kf, scoring='roc_auc')\n",
    "for i, score in enumerate(scores):\n",
    "    print(f'Fold {i+1}:  = {score:.2f}')\n",
    "avg_score = np.mean(scores)\n",
    "print(f'Average SCORE: {avg_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "cat_model = CatBoostClassifier(random_state=42, verbose=0)\n",
    "\n",
    "param_distributions = {\n",
    "    'learning_rate': uniform(0, 1),\n",
    "    'depth': randint(4, 10),\n",
    "    'l2_leaf_reg': uniform(0, 10),\n",
    "    'n_estimators': randint(100, 1001),\n",
    "    'border_count': randint(32, 255),\n",
    "    'min_data_in_leaf': randint(1, 50),\n",
    "    'bagging_temperature': uniform(0, 1),\n",
    "    'random_strength': randint(1, 20),\n",
    "}\n",
    "random_search_cat = RandomizedSearchCV(\n",
    "    estimator=cat_model, \n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=200, \n",
    "    scoring='roc_auc',  # Change scoring if needed\n",
    "    cv=5, \n",
    "    random_state=38, \n",
    "    n_jobs=-1, \n",
    "    verbose=1,\n",
    "    error_score=\"raise\"\n",
    ")\n",
    "%time random_search_cat.fit(X, y)\n",
    "\n",
    "# Print the best score\n",
    "print(\"Best Score:\", random_search_cat.best_score_)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", random_search_cat.best_params_)\n",
    "\n",
    "np.random.seed(42)\n",
    "cat_model = CatBoostClassifier(iterations=612,\n",
    "                          learning_rate=0.030,\n",
    "                          depth=6,\n",
    "                          eval_metric='Logloss',\n",
    "                          random_seed=35,\n",
    "                          bagging_temperature=0.743,\n",
    "                          border_count=99,\n",
    "                          l2_leaf_reg=9.279,\n",
    "                          min_data_in_leaf=19,\n",
    "                          random_strength=11,\n",
    "                          od_type='Iter',\n",
    "                          metric_period=50,\n",
    "                          od_wait=20,verbose=0\n",
    "                        )\n",
    "%time scores=cross_val_score(cat_model, X, y, cv=kf, scoring='roc_auc',error_score=\"raise\")\n",
    "for i, score in enumerate(scores):\n",
    "    print(f'Fold {i+1}:  = {score:.2f}')\n",
    "avg_score = np.mean(scores)\n",
    "print(f'Average SCORE: {avg_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gdbt_model = GradientBoostingClassifier(random_state=42, verbose=0)\n",
    "\n",
    "param_distributions = {\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split':randint(2,20), \n",
    "    'n_estimators': randint(100, 1001),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "}\n",
    "random_search_gdbt = RandomizedSearchCV(\n",
    "    estimator=gdbt_model, \n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=25, \n",
    "    scoring='roc_auc', \n",
    "    cv=5, \n",
    "    random_state=34, \n",
    "    n_jobs=-1, \n",
    "    verbose=1,\n",
    "    error_score=\"raise\"\n",
    ")\n",
    "\n",
    "%time random_search_gdbt.fit(X, y)\n",
    "# Print the best score\n",
    "print(\"Best Score:\", random_search_gdbt.best_score_)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", random_search_gdbt.best_params_)\n",
    "\n",
    "np.random.seed(25)\n",
    "gdbt_model = GradientBoostingClassifier(learning_rate=0.02, max_depth=8, min_samples_leaf=18, min_samples_split=3, \n",
    "                    n_estimators=617, verbose=0)\n",
    "%time scores=cross_val_score(gdbt_model, X, y, cv=kf, scoring='roc_auc',error_score=\"raise\")\n",
    "for i, score in enumerate(scores):\n",
    "    print(f'Fold {i+1}:  = {score:.2f}')\n",
    "avg_score = np.mean(scores)\n",
    "print(f'Average SCORE: {avg_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "extra_model = ExtraTreesClassifier(random_state=33)\n",
    "\n",
    "param_distributions={\n",
    "    \"n_estimators\":randint(100,1000),\n",
    "    \"min_samples_split\":randint(2,20),\n",
    "    \"min_samples_leaf\":randint(1,20),\n",
    "    \"min_weight_fraction_leaf\":uniform(0,0.5),\n",
    "    \"min_impurity_decrease\":uniform(0,1),\n",
    "}\n",
    "random_search_ext = RandomizedSearchCV(estimator=extra_model, param_distributions=param_distributions,n_iter=500, \n",
    "                           scoring='roc_auc', cv=5, random_state=35, n_jobs=-1, verbose=1,error_score=\"raise\")\n",
    "%time random_search_ext.fit(X,y)\n",
    "# Print the best score\n",
    "print(\"Best Score:\", random_search_ext.best_score_)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", random_search_ext.best_params_)\n",
    "\n",
    "np.random.seed(34)\n",
    "ext_model = ExtraTreesClassifier(min_impurity_decrease=0.0174,min_samples_leaf=16,min_samples_split=9, \n",
    "                                 min_weight_fraction_leaf=0.1243, n_estimators=272,verbose=0)\n",
    "%time scores=cross_val_score(ext_model, X, y, cv=kf, scoring='roc_auc',error_score=\"raise\")\n",
    "for i, score in enumerate(scores):\n",
    "    print(f'Fold {i+1}:  = {score:.2f}')\n",
    "avg_score = np.mean(scores)\n",
    "print(f'Average SCORE: {avg_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "np.random.seed(34)\n",
    "svm_model = SVC(kernel=\"poly\",probability=True)\n",
    "%time scores=cross_val_score(svm_model, X, y, cv=kf, scoring='roc_auc',error_score=\"raise\")\n",
    "for i, score in enumerate(scores):\n",
    "    print(f'Fold {i+1}:  = {score:.2f}')\n",
    "avg_score = np.mean(scores)\n",
    "print(f'Average SCORE: {avg_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "np.random.seed(34)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=70)\n",
    "%time scores=cross_val_score(knn_model, X, y, cv=kf, scoring='roc_auc',error_score=\"raise\")\n",
    "for i, score in enumerate(scores):\n",
    "    print(f'Fold {i+1}:  = {score:.2f}')\n",
    "avg_score = np.mean(scores)\n",
    "print(f'Average SCORE: {avg_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "np.random.seed(40)\n",
    "log_model = LogisticRegression(max_iter=10000)\n",
    "%time scores=cross_val_score(log_model, X, y, cv=kf, scoring='roc_auc',error_score=\"raise\")\n",
    "for i, score in enumerate(scores):\n",
    "    print(f'Fold {i+1}:  = {score:.2f}')\n",
    "avg_score = np.mean(scores)\n",
    "print(f'Average SCORE: {avg_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "base_models=[\n",
    "    ('RandomForest',rf_model),(\"Catboost\",cat_model),\n",
    "    ('GDBT',gdbt_model),('Ett',extra_model),('SVM',svm_model),('KNN',knn_model),('Logistic',log_model)\n",
    "]\n",
    "stack_clf = StackingClassifier(\n",
    "     estimators=base_models,\n",
    "    final_estimator=xgb_model\n",
    ")\n",
    "%time scores=cross_val_score(stack_clf, X, y, cv=kf, scoring='roc_auc',error_score=\"raise\")\n",
    "for i, score in enumerate(scores):\n",
    "    print(f'Fold {i+1}:  = {score:.2f}')\n",
    "avg_score = np.mean(scores)\n",
    "print(f'Average SCORE: {avg_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf1=xgb_model.fit(X,y)\n",
    "clf2=rf_model.fit(X,y)\n",
    "clf3=cat_model.fit(X,y)\n",
    "clf4=gdbt_model.fit(X,y)\n",
    "clf5=extra_model.fit(X,y)\n",
    "clf6=svm_model.fit(X,y)\n",
    "clf7=knn_model.fit(X,y)\n",
    "clf8=log_model.fit(X,y)\n",
    "weights=[0.2, 0.15, 0.2, 0.1, 0.05,0.05,0.05, 0.2]\n",
    "\n",
    "y_test=test[\"tackleMade\"].values\n",
    "X_test=test.drop([\"tackleMade\"], axis=1).values\n",
    "predictions = np.array([clf1.predict_proba(X_test), clf2.predict_proba(X_test), clf3.predict_proba(X_test),\n",
    "                       clf4.predict_proba(X_test), clf5.predict_proba(X_test),clf6.predict_proba(X_test),\n",
    "                       clf7.predict_proba(X_test), clf8.predict_proba(X_test)])\n",
    "weighted_predictions = np.tensordot(predictions, weights, axes=((0),(0)))\n",
    "final_probabilities = weighted_predictions / weighted_predictions.sum(axis=1, keepdims=True)\n",
    "prob=final_probabilities[:,1]\n",
    "\n",
    "# if prob is same length as test\n",
    "test.loc[:, \"tackleProbability\"] = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
    "predictions=np.where(prob>=0.5,1,0)\n",
    "test = test.copy()  # optional but shuts up SettingWithCopy warnings\n",
    "test.loc[:, \"predictions\"] = predictions\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, predictions)\n",
    "print(f'ROC AUC Score: {roc_auc}')\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(f'F1 Score: {f1}')\n",
    "roc_auc = roc_auc_score(y_test, prob)\n",
    "print(f'ROC AUC Score for Probability: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "table={'Model': [\"XGBoost\",\"Random Forest\",\"CatBoost\",\"Gradient Boosting Decision Tree\",\"Extra Trees\",\"Support Vector Machine\",\"K-Nearest Neighbor\",\"Logistic Regression\",\"Stacked Classifier\",\"Weighted Classifier(Selected)\"],\n",
    "      'ROC-AUC Score':[0.79,0.79,0.80,0.79,0.73,0.73,0.69,0.77,0.77,0.81],\n",
    "      'Training Time(Seconds)':[22.7,141,7.89,448,1.43,26.4,0.494,149,3863,756]}\n",
    "print(pd.DataFrame(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_mat = confusion_matrix(y_test, predictions)\n",
    "confusion_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = xgb_model.feature_importances_\n",
    "train.drop([\"tackleMade\"],axis=1,inplace=True)\n",
    "feature_names = train.columns\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_ids=nfinal_data.groupby('uniqueid')[\"frameId\"].min().reset_index()\n",
    "new_final=pd.merge(nfinal_data, min_ids, on=[\"uniqueid\",\"frameId\"])\n",
    "new_final['iscarrier'] = (new_final['nflId'] == new_final['ballCarrierId']).astype(int)\n",
    "for index, row in new_final.iterrows():\n",
    "    if(row[\"club\"]==row[\"possessionTeam\"]):\n",
    "        new_final.loc[index,\"side\"]=\"Offense\"\n",
    "    else:\n",
    "        new_final.loc[index,\"side\"]=\"Defense\"\n",
    "    if(row[\"displayName\"]==\"football\"):\n",
    "        new_final.loc[index,\"side\"]=\"Football\"\n",
    "new_final.drop([\"displayName\",\"frameId\",\"club\",\"playDirection\",\"event\",\"possessionTeam\",\"defensiveTeam\",\"AbsolutePostPlay\"],axis=1,inplace=True)\n",
    "plays=plays[plays[\"playNullifiedByPenalty\"]=='N']\n",
    "plays[\"uniqueid\"]=plays[\"gameId\"]*plays[\"playId\"]\n",
    "plays=plays[['uniqueid','prePenaltyPlayResult']]\n",
    "new_final=pd.merge(new_final, plays, on=\"uniqueid\",how=\"left\")\n",
    "new_final[\"AbsolutePostPlay\"]=new_final[\"absoluteYardlineNumber\"]+new_final[\"prePenaltyPlayResult\"]\n",
    "new_final.drop([\"prePenaltyPlayResult\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_final.drop([\"nflId\",\"ballCarrierId\",\"tackleOpportunity\"],axis=1,inplace=True)\n",
    "new_final['idx'] = new_final.groupby(['uniqueid']).cumcount() + 1\n",
    "new_final = new_final[~new_final['uniqueid'].isin([127391783463,3043263972035,6005641766850])]\n",
    "def calculate_distance(group):\n",
    "    ball_carrier_x = group.loc[group['iscarrier'] == 1, 'x'].iloc[0]\n",
    "    ball_carrier_y = group.loc[group['iscarrier'] == 1, 'y'].iloc[0]\n",
    "    group['distance_to_carrier'] = np.sqrt((group['x'] - ball_carrier_x) ** 2 + (group['y'] - ball_carrier_y) ** 2)\n",
    "    return group\n",
    "\n",
    "new_final = new_final.groupby('uniqueid').apply(calculate_distance).reset_index(drop=True)\n",
    "new_final['side'] = pd.Categorical(new_final['side'], categories=[\"Offense\", \"Defense\", \"Football\"], ordered=True)\n",
    "\n",
    "sorted_final = new_final.sort_values(by=['uniqueid', 'side', 'distance_to_carrier'])\n",
    "sorted_final['idx'] = sorted_final.groupby(['uniqueid']).cumcount() + 1\n",
    "sorted_final.drop([\"defenseWinProbability\",\"iscarrier\",\"side\"],axis=1,inplace=True)\n",
    "pivot_dataf = sorted_final.pivot(index=['uniqueid','week_x','absoluteYardlineNumber','AbsolutePostPlay',\n",
    "                                   'quarter','down','yardsToGo','offenseFormation','expectedPoints',\n",
    "                                   'offenseScore','defenseScore','defendersInTheBox',\n",
    "                                   'passProbability','offenseWinProbability','gameClock'], columns='idx')\n",
    "pivot_dataf.columns = [f'{col[0]}{col[1]}' for col in pivot_dataf.columns]\n",
    "pivot_dataf.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_seconds(time_str):\n",
    "    if pd.isna(time_str): \n",
    "        return None\n",
    "    minutes, seconds = map(int, time_str.split(':'))\n",
    "    return minutes * 60 + seconds\n",
    "pivot_dataf['gameClock'] = pivot_dataf['gameClock'].apply(convert_to_seconds)\n",
    "pivot_dataf['TD'] = np.where(pivot_dataf['AbsolutePostPlay'] >= 110, 1, 0)\n",
    "pivot_dataf.drop([\"dir23\",\"o23\"],axis=1,inplace=True)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder=LabelEncoder()\n",
    "pivot_dataf[\"offenseFormation\"]=label_encoder.fit_transform(pivot_dataf[\"offenseFormation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2=pivot_dataf[pivot_dataf[\"week_x\"]>=5]\n",
    "test2=pivot_dataf[pivot_dataf[\"week_x\"]<=4]\n",
    "y2=train2[\"TD\"].values\n",
    "X2=train2.drop([\"AbsolutePostPlay\",\"TD\",\"uniqueid\"],axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic', \n",
    "    eval_metric='logloss'  \n",
    ")\n",
    "param_distributions={\n",
    "    'n_estimators':randint(50,1001),\n",
    "    'learning_rate':uniform(0.01,0.59),\n",
    "    'max_depth':randint(3,11),\n",
    "    'min_child_weight':randint(1,11),\n",
    "    'subsample':uniform(0.5,0.5),\n",
    "    'colsample_bytree':uniform(0.5,0.5),\n",
    "    'gamma':uniform(0.1,0.6),\n",
    "    'reg_alpha':randint(1,8)\n",
    "}\n",
    "random_search_xgb = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_distributions,n_iter=400, \n",
    "                           scoring='roc_auc', cv=5, random_state=39, n_jobs=-1, verbose=1,error_score=\"raise\")\n",
    "random_search_xgb.fit(X2,y2)\n",
    "# Print the best score\n",
    "print(\"Best Score:\", random_search_xgb.best_score_)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", random_search_xgb.best_params_)\n",
    "\n",
    "kf=KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "xgb_model=xgb.XGBClassifier(\n",
    "    n_estimators=965,\n",
    "    learning_rate=0.021,\n",
    "    max_depth=9,\n",
    "    min_child_weight=2,\n",
    "    subsample=0.869,\n",
    "    colsample_bytree=0.909,\n",
    "    gamma=0.239,\n",
    "    reg_alpha=1,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "%time scores=cross_val_score(xgb_model, X2, y2, cv=kf, scoring='roc_auc')\n",
    "\n",
    "for i, score in enumerate(scores):\n",
    "    print(f'Fold {i+1}:  = {score:.2f}')\n",
    "avg_score = np.mean(scores)\n",
    "print(f'Average SCORE: {avg_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~np.isnan(X2).any(axis=1)\n",
    "\n",
    "X2 = X2[mask]\n",
    "y2 = y2[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 1001),\n",
    "    'max_depth': randint(10, 100),\n",
    "    'min_samples_split': randint(2, 11),\n",
    "    'min_samples_leaf': randint(1, 11),\n",
    "    'bootstrap': [True, False],\n",
    "}\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    estimator=rf_model, \n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50, \n",
    "    scoring='roc_auc',\n",
    "    cv=5, \n",
    "    random_state=46, \n",
    "    n_jobs=-1, \n",
    "    verbose=1\n",
    ")\n",
    "%time random_search_rf.fit(X2,y2)\n",
    "# Print the best score\n",
    "print(\"Best Score:\", random_search_rf.best_score_)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", random_search_rf.best_params_)\n",
    "np.random.seed(42)\n",
    "kf=KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rf_model = RandomForestClassifier(bootstrap=False, min_samples_leaf=3, min_samples_split=9, n_estimators=460,random_state=42, max_depth=10,\n",
    "                                )\n",
    "\n",
    "%time scores=cross_val_score(rf_model, X22, y2, cv=kf, scoring='roc_auc')\n",
    "for i, score in enumerate(scores):\n",
    "    print(f'Fold {i+1}:  = {score:.2f}')\n",
    "avg_score = np.mean(scores)\n",
    "print(f'Average SCORE: {avg_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cat_model = CatBoostClassifier(random_state=42, verbose=0)\n",
    "param_distributions = {\n",
    "    'learning_rate': uniform(0, 1),\n",
    "    'depth': randint(4, 10),\n",
    "    'l2_leaf_reg': uniform(0, 10),\n",
    "    'n_estimators': randint(100, 1001),\n",
    "    'border_count': randint(32, 255),\n",
    "    'min_data_in_leaf': randint(1, 50),\n",
    "    'bagging_temperature': uniform(0, 1),\n",
    "    'random_strength': randint(1, 20),\n",
    "}\n",
    "random_search_cat = RandomizedSearchCV(\n",
    "    estimator=cat_model, \n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=200, \n",
    "    scoring='roc_auc',  # Change scoring if needed\n",
    "    cv=5, \n",
    "    random_state=35, \n",
    "    n_jobs=-1, \n",
    "    verbose=1,\n",
    "    error_score=\"raise\"\n",
    ")\n",
    "%time random_search_cat.fit(X2, y2)\n",
    "\n",
    "# Print the best score\n",
    "print(\"Best Score:\", random_search_cat.best_score_)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", random_search_cat.best_params_)\n",
    "\n",
    "np.random.seed(42)\n",
    "cat_model = CatBoostClassifier(iterations=877,\n",
    "                          learning_rate=0.007,\n",
    "                          depth=8,\n",
    "                          eval_metric='Logloss',\n",
    "                          random_seed=35,\n",
    "                          bagging_temperature=0.825,\n",
    "                          border_count=187,\n",
    "                          l2_leaf_reg=6.951,\n",
    "                          min_data_in_leaf=31,\n",
    "                          random_strength=1,\n",
    "                          od_type='Iter',\n",
    "                          metric_period=50,\n",
    "                          od_wait=20,verbose=0\n",
    "                        )\n",
    "\n",
    "%time scores=cross_val_score(cat_model, X22, y2, cv=kf, scoring='roc_auc',error_score=\"raise\")\n",
    "for i, score in enumerate(scores):\n",
    "    print(f'Fold {i+1}:  = {score:.2f}')\n",
    "avg_score = np.mean(scores)\n",
    "print(f'Average SCORE: {avg_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gdbt_model = GradientBoostingClassifier(random_state=42, verbose=0)\n",
    "param_distributions = {\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'min_samples_split':randint(2,20), \n",
    "    'n_estimators': randint(100, 1001),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "}\n",
    "random_search_gdbt = RandomizedSearchCV(\n",
    "    estimator=gdbt_model, \n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20, \n",
    "    scoring='roc_auc', \n",
    "    cv=5, \n",
    "    random_state=34, \n",
    "    n_jobs=-1, \n",
    "    verbose=1,\n",
    "    error_score=\"raise\"\n",
    ")\n",
    "\n",
    "%time random_search_gdbt.fit(X2, y2)\n",
    "# Print the best score\n",
    "print(\"Best Score:\", random_search_gdbt.best_score_)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", random_search_gdbt.best_params_)\n",
    "\n",
    "np.random.seed(25)\n",
    "gdbt_model = GradientBoostingClassifier(learning_rate=0.01, max_depth=5, min_samples_leaf=15, min_samples_split=8, \n",
    "                    n_estimators=473, verbose=0)\n",
    "\n",
    "%time scores=cross_val_score(gdbt_model, X22, y2, cv=kf, scoring='roc_auc',error_score=\"raise\")\n",
    "for i, score in enumerate(scores):\n",
    "    print(f'Fold {i+1}:  = {score:.2f}')\n",
    "avg_score = np.mean(scores)\n",
    "print(f'Average SCORE: {avg_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "extra_model = ExtraTreesClassifier(random_state=33)\n",
    "param_distributions={\n",
    "    \"n_estimators\":randint(100,1000),\n",
    "    \"min_samples_split\":randint(2,20),\n",
    "    \"min_samples_leaf\":randint(1,20),\n",
    "    \"min_weight_fraction_leaf\":uniform(0,0.5),\n",
    "    \"min_impurity_decrease\":uniform(0,1),\n",
    "}\n",
    "random_search_ext = RandomizedSearchCV(estimator=extra_model, param_distributions=param_distributions,n_iter=100, \n",
    "                           scoring='roc_auc', cv=5, random_state=33, n_jobs=-1, verbose=1,error_score=\"raise\")\n",
    "%time random_search_ext.fit(X2,y2)\n",
    "# Print the best score\n",
    "print(\"Best Score:\", random_search_ext.best_score_)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", random_search_ext.best_params_)\n",
    "\n",
    "np.random.seed(34)\n",
    "ext_model = ExtraTreesClassifier(min_impurity_decrease=0.0102,min_samples_leaf=1,min_samples_split=10, \n",
    "                                 min_weight_fraction_leaf=0.1086, n_estimators=960,verbose=0)\n",
    "\n",
    "%time scores=cross_val_score(ext_model, X22, y2, cv=kf, scoring='roc_auc',error_score=\"raise\")\n",
    "for i, score in enumerate(scores):\n",
    "    print(f'Fold {i+1}:  = {score:.2f}')\n",
    "avg_score = np.mean(scores)\n",
    "print(f'Average SCORE: {avg_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "np.random.seed(34)\n",
    "svm_model = SVC(kernel=\"poly\",probability=True)\n",
    "\n",
    "%time scores=cross_val_score(svm_model, X22, y2, cv=kf, scoring='roc_auc',error_score=\"raise\")\n",
    "for i, score in enumerate(scores):\n",
    "    print(f'Fold {i+1}:  = {score:.2f}')\n",
    "avg_score = np.mean(scores)\n",
    "print(f'Average SCORE: {avg_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "np.random.seed(34)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=70)\n",
    "\n",
    "%time scores=cross_val_score(knn_model, X22, y2, cv=kf, scoring='roc_auc',error_score=\"raise\")\n",
    "for i, score in enumerate(scores):\n",
    "    print(f'Fold {i+1}:  = {score:.2f}')\n",
    "avg_score = np.mean(scores)\n",
    "print(f'Average SCORE: {avg_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "np.random.seed(40)\n",
    "log_model = LogisticRegression(max_iter=10000)\n",
    "\n",
    "%time scores=cross_val_score(log_model, X22, y2, cv=kf, scoring='roc_auc',error_score=\"raise\")\n",
    "for i, score in enumerate(scores):\n",
    "    print(f'Fold {i+1}:  = {score:.2f}')\n",
    "avg_score = np.mean(scores)\n",
    "print(f'Average SCORE: {avg_score:.2f}')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "base_models=[\n",
    "    ('RandomForest',rf_model),(\"Catboost\",cat_model),\n",
    "    ('GDBT',gdbt_model),('Ett',extra_model),('SVM',svm_model),('KNN',knn_model),('Logistic',log_model)\n",
    "]\n",
    "stack_clf = StackingClassifier(\n",
    "     estimators=base_models,\n",
    "    final_estimator=xgb_model\n",
    ")\n",
    "\n",
    "%time scores=cross_val_score(stack_clf, X2, y2, cv=kf, scoring='roc_auc',error_score=\"raise\")\n",
    "for i, score in enumerate(scores):\n",
    "    print(f'Fold {i+1}:  = {score:.2f}')\n",
    "avg_score = np.mean(scores)\n",
    "print(f'Average SCORE: {avg_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf11=xgb_model.fit(X2,y2)\n",
    "clf21=rf_model.fit(X2,y2)\n",
    "clf31=cat_model.fit(X2,y2)\n",
    "clf41=gdbt_model.fit(X2,y2)\n",
    "clf51=extra_model.fit(X2,y2)\n",
    "clf61=svm_model.fit(X2,y2)\n",
    "clf71=knn_model.fit(X2,y2)\n",
    "clf81=log_model.fit(X2,y2)\n",
    "weights=[0.2, 0.15, 0.15, 0.15, 0.05,0.05,0.05, 0.2]\n",
    "y_test2=test2[\"TD\"].values\n",
    "X_test2=test2.drop([\"TD\",\"uniqueid\",\"AbsolutePostPlay\"], axis=1).values\n",
    "predictions2 = np.array([clf11.predict_proba(X_test2), clf21.predict_proba(X_test2), clf31.predict_proba(X_test2),\n",
    "                       clf41.predict_proba(X_test2), clf51.predict_proba(X_test2),clf61.predict_proba(X_test2),\n",
    "                       clf71.predict_proba(X_test2), clf81.predict_proba(X_test2)])\n",
    "weighted_predictions2 = np.tensordot(predictions2, weights, axes=((0),(0)))\n",
    "final_probabilities2 = weighted_predictions2 / weighted_predictions2.sum(axis=1, keepdims=True)\n",
    "\n",
    "prob2=final_probabilities2[:,1]\n",
    "predictions2=np.where(prob2>=0.25,1,0)\n",
    "test2.loc[:, \"predictions\"] = predictions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "#test2[test2[\"predictions\"]==1]\n",
    "confusion_mat = confusion_matrix(y_test2, predictions2)\n",
    "confusion_mat\n",
    "#f1_score(y_test2, predictions2)\n",
    "#roc_auc_score(y_test2, prob22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train3=train2[train2[\"TD\"]==0]\n",
    "y3=train3[\"AbsolutePostPlay\"].values\n",
    "\n",
    "X3=train3.drop([\"AbsolutePostPlay\",\"TD\",\"uniqueid\"],axis=1).values\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror',eval_metric='rmse')\n",
    "\n",
    "\n",
    "# Grid Search for xgboost\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "param_distributions={\n",
    "    'n_estimators':randint(50,1001),\n",
    "    'learning_rate':uniform(0.01,0.59),\n",
    "    'max_depth':randint(3,11),\n",
    "    'min_child_weight':randint(1,11),\n",
    "    'subsample':uniform(0.5,0.5),\n",
    "    'colsample_bytree':uniform(0.5,0.5),\n",
    "    'gamma':uniform(0.1,0.6),\n",
    "    'reg_alpha':randint(1,8)\n",
    "}\n",
    "random_search_xgb = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_distributions,n_iter=150, \n",
    "                           scoring='neg_mean_squared_error', cv=5, random_state=25, n_jobs=-1, verbose=1,error_score=\"raise\")\n",
    "\n",
    "%time random_search_xgb.fit(X3,y3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~np.isnan(X3).any(axis=1)\n",
    "\n",
    "X3 = X3[mask]\n",
    "y3 = y3[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf=KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "xgb_model=xgb.XGBRegressor(\n",
    "    n_estimators=279,\n",
    "    learning_rate=0.038,\n",
    "    max_depth=3,\n",
    "    min_child_weight=9,\n",
    "    subsample=0.608,\n",
    "    colsample_bytree=0.571,\n",
    "    gamma=0.445,\n",
    "    reg_alpha=4,\n",
    "    reg_lambda=1,\n",
    "    objective='reg:squarederror',\n",
    "    eval_metric='rmse'\n",
    ")\n",
    "\n",
    "\n",
    "%time scores=cross_val_score(xgb_model, X3, y3, cv=kf, scoring='neg_mean_squared_error')\n",
    "\n",
    "rmse = np.sqrt(-scores)\n",
    "\n",
    "# Display the RMSE for each fold\n",
    "for i, score in enumerate(rmse):\n",
    "    print(f'Fold {i+1}: RMSE = {score:.2f}')\n",
    "\n",
    "# Calculate and display the average RMSE across all folds\n",
    "avg_rmse = np.mean(rmse)\n",
    "print(f'Average RMSE: {avg_rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(criterion=\"squared_error\",random_state=42,n_jobs=-1)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "param_distributions={\n",
    "    'n_estimators':randint(50,1001),\n",
    "    'max_depth':randint(10,100),\n",
    "    'min_samples_split':randint(2,11),\n",
    "    'min_samples_leaf':randint(1,11),\n",
    "    'bootstrap':[True, False],\n",
    "}\n",
    "random_search_rf = RandomizedSearchCV(estimator=rf_model, param_distributions=param_distributions,n_iter=15, \n",
    "                           scoring='neg_mean_squared_error', cv=3, random_state=25, n_jobs=-1, verbose=1,error_score=\"raise\")\n",
    "\n",
    "%time random_search_rf.fit(X3,y3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters:\",random_search_rf.best_params_)\n",
    "print(\"Best score:\",-random_search_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Best parameters:\",random_search_rf.best_params_)\n",
    "#print(\"Best score:\",-random_search_rf.best_score_)\n",
    "# Setup random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize RandomForestRegressor\n",
    "rf_regressor = RandomForestRegressor(bootstrap=True, min_samples_leaf=9, min_samples_split=8, n_estimators=549,random_state=42, max_depth=24)\n",
    "\n",
    "# Perform K-Fold CV and calculate RMSE\n",
    "%time mse_scores = cross_val_score(rf_regressor, X3, y3, cv=kf, scoring='neg_mean_squared_error')\n",
    "rmse_scores = np.sqrt(-mse_scores)\n",
    "\n",
    "# Display the RMSE for each fold\n",
    "for i, rmse in enumerate(rmse_scores, 1):\n",
    "    print(f'Fold {i}: RMSE = {rmse:.2f}')\n",
    "\n",
    "# Calculate and display the average RMSE\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "print(f'Average RMSE: {avg_rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_model=Ridge(alpha=1)\n",
    "\n",
    "%time mse_scores = cross_val_score(ridge_model, X3, y3, cv=kf, scoring='neg_mean_squared_error')\n",
    "rmse_scores = np.sqrt(-mse_scores)\n",
    "\n",
    "# Display the RMSE for each fold\n",
    "for i, rmse in enumerate(rmse_scores, 1):\n",
    "    print(f'Fold {i}: RMSE = {rmse:.2f}')\n",
    "\n",
    "# Calculate and display the average RMSE\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "print(f'Average RMSE: {avg_rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_model=Lasso(alpha=0.1)\n",
    "\n",
    "%time mse_scores = cross_val_score(lasso_model, X3, y3, cv=kf, scoring='neg_mean_squared_error')\n",
    "rmse_scores = np.sqrt(-mse_scores)\n",
    "\n",
    "# Display the RMSE for each fold\n",
    "for i, rmse in enumerate(rmse_scores, 1):\n",
    "    print(f'Fold {i}: RMSE = {rmse:.2f}')\n",
    "\n",
    "# Calculate and display the average RMSE\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "print(f'Average RMSE: {avg_rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "cat_model = CatBoostRegressor(random_state=42,verbose=0)\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "param_distributions={\n",
    "    'learning_rate':uniform(0,1),\n",
    "    'depth':randint(4,10),\n",
    "    'l2_leaf_reg':uniform(0,10),\n",
    "    'n_estimators':randint(100,1001),\n",
    "    'border_count':randint(32,255),\n",
    "    'min_data_in_leaf':randint(1,50),\n",
    "    'bagging_temperature':uniform(0,1),\n",
    "    'random_strength':randint(1,20),\n",
    "}\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "random_search_cat = RandomizedSearchCV(estimator=cat_model, param_distributions=param_distributions,n_iter=80, \n",
    "                           scoring='neg_mean_squared_error', cv=5, random_state=35, n_jobs=-1, verbose=1,error_score=\"raise\")\n",
    "\n",
    "%time random_search_cat.fit(X2,y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters:\",random_search_cat.best_params_)\n",
    "print(\"Best score:\",-random_search_cat.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_model = CatBoostRegressor(iterations=751,\n",
    "                          learning_rate=0.028,\n",
    "                          depth=5,\n",
    "                          eval_metric='RMSE',\n",
    "                          random_seed=35,\n",
    "                          bagging_temperature=0.458,\n",
    "                          border_count=215,\n",
    "                          l2_leaf_reg=2.4611,\n",
    "                          min_data_in_leaf=41,\n",
    "                          random_strength=12,\n",
    "                          od_type='Iter',\n",
    "                          metric_period=50,\n",
    "                          od_wait=20,verbose=0)\n",
    "%time mse_scores = cross_val_score(cat_model, X3, y3, cv=kf, scoring='neg_mean_squared_error')\n",
    "rmse_scores = np.sqrt(-mse_scores)\n",
    "\n",
    "# Display the RMSE for each fold\n",
    "for i, rmse in enumerate(rmse_scores, 1):\n",
    "    print(f'Fold {i}: RMSE = {rmse:.2f}')\n",
    "\n",
    "# Calculate and display the average RMSE\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "print(f'Average RMSE: {avg_rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "extra_model = ExtraTreesRegressor(random_state=42)\n",
    "\n",
    "param_distributions={\n",
    "    \"n_estimators\":randint(100,1000),\n",
    "    \"min_samples_split\":randint(2,20),\n",
    "    \"min_samples_leaf\":randint(1,20),\n",
    "    \"min_weight_fraction_leaf\":uniform(0,0.5),\n",
    "    \"min_impurity_decrease\":uniform(0,1),\n",
    "}\n",
    "\n",
    "random_search_ext = RandomizedSearchCV(estimator=extra_model, param_distributions=param_distributions,n_iter=160, \n",
    "                           scoring='neg_mean_squared_error', cv=5, random_state=35, n_jobs=-1, verbose=1,error_score=\"raise\")\n",
    "\n",
    "%time random_search_ext.fit(X2,y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Best parameters:\",random_search_ext.best_params_)\n",
    "print(\"Best score:\",-random_search_ext.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extra_model=ExtraTreesRegressor(n_estimators=594, random_state=35,min_impurity_decrease=0.019, min_samples_leaf=5,\n",
    "                               min_samples_split=6,min_weight_fraction_leaf=0.0057)\n",
    "%time mse_scores = cross_val_score(extra_model, X3, y3, cv=kf, scoring='neg_mean_squared_error')\n",
    "rmse_scores = np.sqrt(-mse_scores)\n",
    "\n",
    "# Display the RMSE for each fold\n",
    "for i, rmse in enumerate(rmse_scores, 1):\n",
    "    print(f'Fold {i}: RMSE = {rmse:.2f}')\n",
    "\n",
    "# Calculate and display the average RMSE\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "print(f'Average RMSE: {avg_rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic_net_model = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "%time mse_scores = cross_val_score(elastic_net_model, X3, y3, cv=kf, scoring='neg_mean_squared_error')\n",
    "rmse_scores = np.sqrt(-mse_scores)\n",
    "\n",
    "# Display the RMSE for each fold\n",
    "for i, rmse in enumerate(rmse_scores, 1):\n",
    "    print(f'Fold {i}: RMSE = {rmse:.2f}')\n",
    "\n",
    "# Calculate and display the average RMSE\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "print(f'Average RMSE: {avg_rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr_model = SVR(C=1.0, epsilon=0.1, kernel='rbf')\n",
    "%time mse_scores = cross_val_score(svr_model, X3, y3, cv=kf, scoring='neg_mean_squared_error')\n",
    "rmse_scores = np.sqrt(-mse_scores)\n",
    "\n",
    "# Display the RMSE for each fold\n",
    "for i, rmse in enumerate(rmse_scores, 1):\n",
    "    print(f'Fold {i}: RMSE = {rmse:.2f}')\n",
    "\n",
    "# Calculate and display the average RMSE\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "print(f'Average RMSE: {avg_rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "base_learners = [\n",
    "    ('ridge', ridge_model),\n",
    "    ('lasso', xgb_model),\n",
    "    ('rf', rf_regressor),\n",
    "    ('extra', extra_model),\n",
    "    ('catboost',cat_model),\n",
    "    ('elasticnet',elastic_net_model)\n",
    "]\n",
    "\n",
    "stack_model=StackingRegressor(estimators=base_learners, \n",
    "                             final_estimator=lasso_model)\n",
    "%time mse_scores = cross_val_score(stack_model, X3, y3, cv=kf, scoring='neg_mean_squared_error')\n",
    "rmse_scores = np.sqrt(-mse_scores)\n",
    "\n",
    "# Display the RMSE for each fold\n",
    "for i, rmse in enumerate(rmse_scores, 1):\n",
    "    print(f'Fold {i}: RMSE = {rmse:.2f}')\n",
    "\n",
    "# Calculate and display the average RMSE\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "print(f'Average RMSE: {avg_rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_model.fit(X3,y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "test3=test2[test2[\"TD\"]==0]\n",
    "test3 = test3[train3.columns]  \n",
    "X_test3=test3.drop([\"uniqueid\",\"AbsolutePostPlay\",\"TD\"],axis=1).values\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pred3 = stack_model.predict(X_test3)\n",
    "test3.loc[:, \"predictions\"] = pred3\n",
    "\n",
    "test3['residuals'] =  test3['predictions'] - test3['AbsolutePostPlay']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(test3['predictions'], test3['residuals'])\n",
    "plt.title('Residuals vs. Predictions')\n",
    "plt.xlabel('Predictions (pred3)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(test3['residuals'], kde=True)\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "made_tackle=data_final[(data_final[\"tackleMade\"]==1) & (data_final[\"week_x\"]<=4)]\n",
    "nfinal_data.drop([\"AbsolutePostPlay\"], axis=1, inplace=True)\n",
    "nfinal_data=pd.merge(nfinal_data,plays,on=\"uniqueid\",how=\"left\")\n",
    "nfinal_data[\"AbsolutePostplay\"]=nfinal_data[\"absoluteYardlineNumber\"]+nfinal_data[\"prePenaltyPlayResult\"]\n",
    "nfinal_data.drop([\"prePenaltyPlayResult\"],axis=1,inplace=True)\n",
    "combos=made_tackle[['uniqueid','frameId','tacklerNflId']].drop_duplicates()\n",
    "nfinal_data=nfinal_data.merge(combos, on=[\"uniqueid\",\"frameId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in nfinal_data.iterrows():\n",
    "    if(row[\"club\"]==row[\"possessionTeam\"]):\n",
    "        nfinal_data.loc[index,\"side\"]=\"Offense\"\n",
    "    else:\n",
    "        nfinal_data.loc[index,\"side\"]=\"Defense\"\n",
    "    if(row[\"displayName\"]==\"football\"):\n",
    "        nfinal_data.loc[index,\"side\"]=\"Football\"\n",
    "nfinal_data['iscarrier'] = (nfinal_data['nflId'] == nfinal_data['ballCarrierId']).astype(int)\n",
    "def calculate_distance(group):\n",
    "    ball_carrier_x = group.loc[group['iscarrier'] == 1, 'x'].iloc[0]\n",
    "    ball_carrier_y = group.loc[group['iscarrier'] == 1, 'y'].iloc[0]\n",
    "    group['distance_to_carrier'] = np.sqrt((group['x'] - ball_carrier_x) ** 2 + (group['y'] - ball_carrier_y) ** 2)\n",
    "    return group\n",
    "nfinal_data = nfinal_data.groupby('uniqueid').apply(calculate_distance).reset_index(drop=True)\n",
    "nfinal_data['side'] = pd.Categorical(nfinal_data['side'], categories=[\"Offense\", \"Defense\", \"Football\"], ordered=True)\n",
    "sorted_final2 = nfinal_data.sort_values(by=['uniqueid', 'side', 'distance_to_carrier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_coordinates(group):\n",
    "    row_22_x = group.iloc[21]['x']\n",
    "    row_22_y = group.iloc[21]['y']\n",
    "    group.loc[group['nflId'] == group['tacklerNflId'], 'x'] = row_22_x+0.05\n",
    "    group.loc[group['nflId'] == group['tacklerNflId'], 'y'] = row_22_y-0.05\n",
    "    return group\n",
    "sorted_final2 = sorted_final2.groupby(\"uniqueid\").apply(update_coordinates)\n",
    "sorted_final2=sorted_final2.reset_index(level=\"uniqueid\",drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_seconds(time_str):\n",
    "    if pd.isna(time_str):  \n",
    "        return None\n",
    "    minutes, seconds = map(int, time_str.split(':'))\n",
    "    return minutes * 60 + seconds\n",
    "sorted_final2['gameClock'] = sorted_final2['gameClock'].apply(convert_to_seconds)\n",
    "sorted_final2 = sorted_final2.groupby('uniqueid').apply(calculate_distance).reset_index(drop=True)\n",
    "sorted_final3 = sorted_final2.sort_values(by=['uniqueid', 'side', 'distance_to_carrier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sorted_final3[\"offenseFormation\"]=label_encoder.fit_transform(sorted_final2[\"offenseFormation\"])\n",
    "sorted_final3[\"side\"]=label_encoder.fit_transform(sorted_final2[\"side\"])\n",
    "sorted_final3.drop([\"displayName\",\"nflId\",\"club\",\"playDirection\",\"event\",\"ballCarrierId\",\"tackleOpportunity\",\"possessionTeam\",\"defensiveTeam\"],axis=1,inplace=True)\n",
    "sorted_final3['idx'] = sorted_final3.groupby(['uniqueid']).cumcount() + 1\n",
    "sorted_final2.drop([\"tacklerNflId\"],axis=1,inplace=True)\n",
    "sorted_final3 = sorted_final3.groupby('uniqueid').apply(calculate_distance).reset_index(drop=True)\n",
    "sorted_final2['side'] = pd.Categorical(sorted_final2['side'], categories=[\"Offense\", \"Defense\", \"Football\"], ordered=True)\n",
    "sorted_final2 = sorted_final2.sort_values(by=['uniqueid', 'side', 'distance_to_carrier'])\n",
    "sorted_final2.drop([\"side\",\"iscarrier\"],axis=1,inplace=True)\n",
    "sorted_final3.drop([\"side\",\"iscarrier\"],axis=1,inplace=True)\n",
    "'''\n",
    "pivot_df2 = sorted_final3.pivot(index=['uniqueid','absoluteYardlineNumber','AbsolutePostplay','frameId',\n",
    "                                   'quarter','down','yardsToGo','offenseFormation',\"expectedPoints\",\"tacklerNflId\",\n",
    "                                   'offenseScore','defenseScore','defendersInTheBox',\n",
    "                                   'passProbability','offenseWinProbability','defenseWinProbability','gameClock'], columns='idx')\n",
    "pivot_df2.columns = [f'{col[0]}{col[1]}' for col in pivot_df2.columns]\n",
    "pivot_df2.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict=pivot_df2.drop([\"AbsolutePostplay\",\"frameId\",\"defenseWinProbability\",\"uniqueid\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "print(train3.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot_df2.drop([\"dir23\",\"o23\"],axis=1, inplace=True)\n",
    "#y_true=pivot_df2[\"AbsolutePostplay\"].values\n",
    "X_predict=pivot_df2.drop([\"AbsolutePostplay\",\"frameId\",\"defenseWinProbability\",\"uniqueid\",\"tacklerNflId\"],axis=1).values\n",
    "predTD = np.array([clf11.predict_proba(X_predict), clf21.predict_proba(X_predict), clf31.predict_proba(X_predict),\n",
    "                       clf41.predict_proba(X_predict), clf51.predict_proba(X_predict),clf61.predict_proba(X_predict),\n",
    "                       clf71.predict_proba(X_predict), clf81.predict_proba(X_predict)])\n",
    "weighted_predictions3 = np.tensordot(predTD, weights, axes=((0),(0)))\n",
    "final_probabilities3 = weighted_predictions3 / weighted_predictions3.sum(axis=1, keepdims=True)\n",
    "\n",
    "prob3=final_probabilities3[:,1]\n",
    "predictions3=np.where(prob3>=0.25,1,0)\n",
    "pivot_df2[\"TD\"]=predictions3\n",
    "\n",
    "y_predict=stack_model.predict(X_predict)\n",
    "pivot_df2[\"withoutTackler\"]=y_predict\n",
    "pivot_df2[\"yardssaved\"]=pivot_df2[\"withoutTackler\"]-pivot_df2[\"AbsolutePostplay\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df3=pivot_df2[pivot_df2[\"TD\"]==0]\n",
    "predYards=pivot_df3.drop([\"TD\",\"AbsolutePostplay\",\"uniqueid\",\"frameId\",\"defenseWinProbability\",\"tacklerNflId\",\"yardssaved\",\"withoutTackler\"],axis=1).values\n",
    "res=stack_model.predict(predYards)\n",
    "pivot_df3[\"withoutTackler\"]=res\n",
    "TDs=pivot_df2[pivot_df2[\"TD\"]==1]\n",
    "TDs[\"withoutTackler\"]=110\n",
    "FINAL=pd.concat([TDs,pivot_df3])\n",
    "FINAL[\"yardssaved\"]=FINAL[\"withoutTackler\"]-FINAL[\"AbsolutePostplay\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plays[plays[\"uniqueid\"]==6573818175853]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"AbsolutePostplay\"]=test[\"absoluteYardlineNumber\"]+test[\"prePenaltyPlayResult\"]\n",
    "final_result=pd.merge(test,FINAL[[\"uniqueid\",\"withoutTackler\",\"yardssaved\"]],on=\"uniqueid\",how=\"left\")\n",
    "final_result.loc[final_result['tackleMade'] == 0, [\"yardssaved\"]] = np.nan\n",
    "final_result.loc[final_result['tackleMade'] == 0, [\"withoutTackler\"]] = np.nan\n",
    "final_result[\"potentialSaved\"]=final_result[\"AbsolutePostplay\"]-final_result[\"x1\"]-3\n",
    "final_result.loc[final_result['tackleMade'] == 1, [\"potentialSaved\"]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result['1Dsaved'] = np.where(\n",
    "    (final_result['withoutTackler'] - final_result['absoluteYardlineNumber'] > final_result['yardsToGo']) &\n",
    "    (final_result['AbsolutePostplay'] - final_result['absoluteYardlineNumber'] < final_result['yardsToGo']),\n",
    "    1, \n",
    "    0  \n",
    ")\n",
    "final_result['TDsaved'] = np.where(\n",
    "    (final_result['withoutTackler'] >=110) &\n",
    "    (final_result['AbsolutePostplay']<110),\n",
    "    1, \n",
    "    0  \n",
    ")\n",
    "final_result['potential1Dsaved'] = np.where(\n",
    "    (final_result['AbsolutePostplay'] -final_result[\"potentialSaved\"]- final_result['absoluteYardlineNumber'] < final_result['yardsToGo']) &\n",
    "    (final_result['AbsolutePostplay'] - final_result['absoluteYardlineNumber'] > final_result['yardsToGo']),\n",
    "    1,  \n",
    "    0  \n",
    ")\n",
    "final_result['potentialTDsaved'] = np.where(\n",
    "    (final_result['AbsolutePostplay']>=110) &\n",
    "    (final_result['AbsolutePostplay'] - final_result['potentialSaved'] < 110),\n",
    "    1, \n",
    "    0   \n",
    ")\n",
    "final_result.loc[final_result['tackleMade'] == 0, [\"1Dsaved\",\"TDsaved\"]] = np.nan\n",
    "final_result.loc[final_result['tackleMade'] == 1, [\"potential1Dsaved\",\"potentialTDsaved\"]] = np.nan\n",
    "final_result[\"tacklerNflId\"]=test_tackler\n",
    "final_result = final_result.rename(columns={'tacklerNflId': 'nflId'})\n",
    "final_res=pd.merge(final_result, players[[\"nflId\",\"displayName\"]], on=\"nflId\",how=\"left\")\n",
    "result=final_res[[\"tackleMade\",\"tackleProbability\",\"displayName\",\"withoutTackler\",\"potentialSaved\",\"yardssaved\",\"1Dsaved\",\"TDsaved\",\"potential1Dsaved\",\"potentialTDsaved\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_impact_score(row):\n",
    "    if row['tackleMade'] == 1:\n",
    "        if row['TDsaved'] == 1:\n",
    "            score = (row['yardssaved'] * 0.2 + 7)\n",
    "        else:\n",
    "            score = (row['yardssaved'] * 0.2 +\n",
    "                     row['1Dsaved'] * 3)\n",
    "    else:\n",
    "        if row['potentialTDsaved'] == 1:\n",
    "            score = (-row['potentialSaved'] * 0.2 - 7)\n",
    "        else:\n",
    "            score = (-row['potentialSaved'] * 0.2 -\n",
    "                     row['potential1Dsaved'] * 3)\n",
    "    return score\n",
    "result['impactScore'] = result.apply(calculate_impact_score, axis=1)\n",
    "\n",
    "def calculate_points_added(row):\n",
    "    if row['impactScore'] > 0:\n",
    "        return row['impactScore'] * (1 + (1 - row['tackleProbability']))\n",
    "    else:\n",
    "        return row['impactScore'] * row['tackleProbability']\n",
    "result['SITR'] = result.apply(calculate_points_added, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = result.groupby('displayName')['SITR']\n",
    "average_points_added = grouped.mean()\n",
    "total_points_added = grouped.sum()\n",
    "combined_stats = pd.DataFrame({\n",
    "    'Average SITR': average_points_added,\n",
    "    'Total SITR': total_points_added\n",
    "}).reset_index()\n",
    "combined_stats[\"tackleOpportunities\"]=combined_stats[\"Total SITR\"]/combined_stats[\"Average SITR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined2=combined_stats[combined_stats[\"tackleOpportunities\"]>7]\n",
    "combined3=pd.merge(combined2, players, on=\"displayName\",how=\"left\")\n",
    "combined3.drop([\"nflId\",\"height\",\"weight\",\"birthDate\",\"collegeName\"],axis=1,inplace=True)\n",
    "combined3.loc[combined3['position'].isin(['QB','OLB','ILB','MLB']), 'position'] = 'LB'\n",
    "combined3.loc[combined3['position'].isin(['G', 'RB','CB','FS','SS','DB']), 'position'] = 'DB'\n",
    "combined3.loc[combined3['position'].isin(['DT','DE','NT']),'position']='DL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined3[combined3[\"position\"]==\"DL\"].sort_values(by=\"Average SITR\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_df\n",
    "dl_df.to_csv(\"dl_df.csv\", index=False)  # index=False avoids extra row numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_df\n",
    "lb_df.to_csv(\"lb_df.csv\", index=False)  # index=False avoids extra row numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import matplotlib.patheffects as pe\n",
    "\n",
    "# -------- data --------\n",
    "df = lb_df.copy()\n",
    "\n",
    "x = df[\"tackleOpportunities\"].astype(float).to_numpy()\n",
    "y = df[\"Average SITR\"].astype(float).to_numpy()\n",
    "tot = df[\"Total SITR\"].astype(float).to_numpy()\n",
    "names = df[\"displayName\"].to_numpy()\n",
    "\n",
    "# bubble size (robust scaling)\n",
    "smin, smax = np.percentile(tot, [5, 95])\n",
    "sizes = 50 + 1500 * (np.clip(tot, smin, smax) - smin) / (smax - smin + 1e-9)\n",
    "\n",
    "# color by Average SITR\n",
    "cmap = plt.get_cmap(\"turbo\") if \"turbo\" in plt.colormaps() else plt.get_cmap(\"plasma\")\n",
    "norm = colors.Normalize(vmin=y.min(), vmax=y.max())\n",
    "\n",
    "# -------- plot --------\n",
    "fig = plt.figure(figsize=(11, 8))\n",
    "ax = plt.gca()\n",
    "\n",
    "sc = ax.scatter(\n",
    "    x, y,\n",
    "    s=sizes,\n",
    "    c=norm(y), cmap=cmap,\n",
    "    alpha=0.85, edgecolor=\"k\", linewidths=0.3\n",
    ")\n",
    "\n",
    "# quadrant lines\n",
    "x_med, y_med = np.median(x), np.median(y)\n",
    "ax.axvline(x_med, linestyle=\"--\", linewidth=1, alpha=0.5)\n",
    "ax.axhline(y_med, linestyle=\"--\", linewidth=1, alpha=0.5)\n",
    "\n",
    "# -------- label top-4 by Total SITR --------\n",
    "top4_idx = np.argsort(-tot)[:4]\n",
    "xr = max(x.max() - x.min(), 1e-9)\n",
    "yr = max(y.max() - y.min(), 1e-9)\n",
    "dx0, dy0 = 0.06 * xr, 0.06 * yr\n",
    "\n",
    "# force NW arrows for these two if they are in the top-4\n",
    "special_nw = {\"Bobby Okereke\", \"T.J. Edwards\"}\n",
    "\n",
    "for i in top4_idx:\n",
    "    name = str(names[i])\n",
    "\n",
    "    # label offset direction\n",
    "    if name in special_nw:\n",
    "        dx, dy = -abs(dx0)-3, abs(dy0)-0.2   # NW\n",
    "    else:\n",
    "        dx = dx0 if x[i] >= x_med else -dx0\n",
    "        dy = dy0 if y[i] >= y_med else -dy0\n",
    "\n",
    "    # arrow start position\n",
    "    if name == \"Pete Werner\":\n",
    "        # compute bubble radius in pixels from scatter size (points^2)\n",
    "        r_pts = np.sqrt(sizes[i] / np.pi)              # points\n",
    "        r_px  = r_pts * (fig.dpi / 72.0)               # pixels\n",
    "        pad_px = 3.0                                   # tiny gap above the circle\n",
    "\n",
    "        # transform (x,y) -> display px, add vertical offset, transform back to data\n",
    "        p_disp = ax.transData.transform((x[i], y[i]-0.15))\n",
    "        p_top_disp = (p_disp[0], p_disp[1] + r_px + pad_px)\n",
    "        x_start, y_start = ax.transData.inverted().transform(p_top_disp)\n",
    "        start_xy = (x_start, y_start)\n",
    "    else:\n",
    "        start_xy = (x[i], y[i])\n",
    "\n",
    "    ann = ax.annotate(\n",
    "        name,\n",
    "        xy=start_xy,                           # arrow tail (starts at top of Pete's bubble)\n",
    "        xytext=(x[i] + dx, y[i] + dy),         # label position\n",
    "        textcoords=\"data\",\n",
    "        fontsize=9,\n",
    "        ha=\"left\", va=\"bottom\",\n",
    "        arrowprops=dict(arrowstyle=\"-\", lw=0.9, alpha=0.9)\n",
    "    )\n",
    "    ann.set_path_effects([pe.withStroke(linewidth=2.5, foreground=\"white\")])\n",
    "\n",
    "# -------- cosmetics --------\n",
    "cb = plt.colorbar(sc, ax=ax, pad=0.015)\n",
    "cb.set_label(\"Average SITR\")\n",
    "cb.remove()\n",
    "\n",
    "ax.set_xlabel(\"Tackle Opportunities\",fontsize=18)\n",
    "ax.set_ylabel(\"Average Solo Tackle Impact Rating\",fontsize=18)\n",
    "ax.set_title(\"Linebackers Tackle Efficiency and Volume\", fontsize=18, pad=10)\n",
    "ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"lb_efficiency_volume.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Optional save:\n",
    "#plt.savefig(\"lb_efficiency_volume.png\", dpi=300, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import matplotlib.patheffects as pe\n",
    "\n",
    "# -------- data --------\n",
    "df = db_df.copy()\n",
    "\n",
    "x = df[\"tackleOpportunities\"].astype(float).to_numpy()\n",
    "y = df[\"Average SITR\"].astype(float).to_numpy()\n",
    "tot = df[\"Total SITR\"].astype(float).to_numpy()\n",
    "names = df[\"displayName\"].to_numpy()\n",
    "\n",
    "# bubble size (robust scaling)\n",
    "smin, smax = np.percentile(tot, [5, 95])\n",
    "sizes = 50 + 1500 * (np.clip(tot, smin, smax) - smin) / (smax - smin + 1e-9)\n",
    "\n",
    "# color by Average SITR\n",
    "cmap = plt.get_cmap(\"turbo\") if \"turbo\" in plt.colormaps() else plt.get_cmap(\"plasma\")\n",
    "norm = colors.Normalize(vmin=y.min(), vmax=y.max())\n",
    "\n",
    "# -------- plot --------\n",
    "fig = plt.figure(figsize=(11, 8))\n",
    "ax = plt.gca()\n",
    "\n",
    "sc = ax.scatter(\n",
    "    x, y,\n",
    "    s=sizes,\n",
    "    c=norm(y), cmap=cmap,\n",
    "    alpha=0.85, edgecolor=\"k\", linewidths=0.3\n",
    ")\n",
    "\n",
    "# quadrant lines\n",
    "x_med, y_med = np.median(x), np.median(y)\n",
    "ax.axvline(x_med, linestyle=\"--\", linewidth=1, alpha=0.5)\n",
    "ax.axhline(y_med, linestyle=\"--\", linewidth=1, alpha=0.5)\n",
    "\n",
    "# -------- label top-4 by Total SITR --------\n",
    "# -------- label top-4 by Total SITR --------\n",
    "top4_idx = np.argsort(-tot)[:4]\n",
    "xr = max(x.max() - x.min(), 1e-9)\n",
    "yr = max(y.max() - y.min(), 1e-9)\n",
    "dx0, dy0 = 0.06 * xr, 0.06 * yr\n",
    "\n",
    "for i in top4_idx:\n",
    "    name = str(names[i])\n",
    "\n",
    "    # vertical offset stays based on median\n",
    "    dy = dy0 if y[i] >= y_med else -dy0\n",
    "\n",
    "    # force horizontal direction for specific players\n",
    "    if name == \"Anthony Brown\":\n",
    "        dx = -abs(dx0)-2  # go left\n",
    "        dy = abs(dy0)-0.5\n",
    "    elif name == \"Nate Hobbs\":\n",
    "        dx =  abs(dx0)  # go right\n",
    "        dy = abs(dy0) - 0.5\n",
    "    else:\n",
    "        dx = dx0 if x[i] >= x_med else -dx0  # default logic\n",
    "\n",
    "    ann = ax.annotate(\n",
    "        name,\n",
    "        xy=(x[i], y[i]),\n",
    "        xytext=(x[i] + dx, y[i] + dy),\n",
    "        textcoords=\"data\",\n",
    "        fontsize=9,\n",
    "        ha=\"left\", va=\"bottom\",\n",
    "        arrowprops=dict(arrowstyle=\"-\", lw=0.9, alpha=0.9)\n",
    "    )\n",
    "    ann.set_path_effects([pe.withStroke(linewidth=2.5, foreground=\"white\")])\n",
    "\n",
    "# -------- cosmetics --------\n",
    "# no colorbar (per your “slide bar” ban)\n",
    "ax.set_xlabel(\"Tackle Opportunities\", fontsize=18)\n",
    "ax.set_ylabel(\"Average Solo Tackle Impact Rating\", fontsize=18)\n",
    "ax.set_title(\"Defensive Backs Tackle Efficiency and Volume\", fontsize=18, pad=10)\n",
    "ax.grid(True, alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"db_efficiency_volume.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Optional save:\n",
    "#plt.savefig(\"db_bubble_top4.png\", dpi=300, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined3[combined3[\"position\"]==\"LB\"].sort_values(by=\"Average SITR\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_df\n",
    "db_df.to_csv(\"db_df.csv\", index=False)  # index=False avoids extra row numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "# Work on a copy\n",
    "df = lb_df.copy()\n",
    "vals = df[\"Average SITR\"].astype(float).dropna().to_numpy()\n",
    "\n",
    "# Figure\n",
    "fig = plt.figure(figsize=(11, 7))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Choose bins\n",
    "bins = 24\n",
    "\n",
    "# Colormap for pretty bars\n",
    "cmap = plt.get_cmap(\"turbo\") if \"turbo\" in plt.colormaps() else plt.get_cmap(\"plasma\")\n",
    "norm = colors.Normalize(vmin=vals.min(), vmax=vals.max())\n",
    "\n",
    "# Histogram\n",
    "n, b, patches = ax.hist(\n",
    "    vals,\n",
    "    bins=bins,\n",
    "    rwidth=0.92,\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.8,\n",
    "    alpha=0.95\n",
    ")\n",
    "\n",
    "# Color each bar by its bin center\n",
    "for i, p in enumerate(patches):\n",
    "    center = 0.5 * (b[i] + b[i+1])\n",
    "    p.set_facecolor(cmap(norm(center)))\n",
    "\n",
    "# Colorbar keyed to Average SITR values\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cb = plt.colorbar(sm, ax=ax, pad=0.015)\n",
    "cb.set_label(\"Average SITR (bin color)\")\n",
    "cb.remove()\n",
    "# Mean and median markers\n",
    "mu = float(np.mean(vals))\n",
    "med = float(np.median(vals))\n",
    "ax.axvline(mu, linestyle=\"--\", linewidth=1.5, label=f\"Mean = {mu:.2f}\")\n",
    "ax.axvline(med, linestyle=\"-.\", linewidth=1.5, label=f\"Median = {med:.2f}\")\n",
    "\n",
    "# Labels/title\n",
    "ax.set_xlabel(\"Average SITR\")\n",
    "ax.set_ylabel(\"Number of Players\")\n",
    "ax.set_title(\"Distribution of SITR for Linebackers\")\n",
    "\n",
    "# Light grid + tidy spines\n",
    "ax.grid(True, alpha=0.18)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_alpha(0.5)\n",
    "\n",
    "ax.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "# Work on a copy\n",
    "df = db_df.copy()\n",
    "vals = df[\"Average SITR\"].astype(float).dropna().to_numpy()\n",
    "\n",
    "# Figure\n",
    "fig = plt.figure(figsize=(11, 7))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Choose bins\n",
    "bins = 24\n",
    "\n",
    "# Colormap for pretty bars\n",
    "cmap = plt.get_cmap(\"turbo\") if \"turbo\" in plt.colormaps() else plt.get_cmap(\"plasma\")\n",
    "norm = colors.Normalize(vmin=vals.min(), vmax=vals.max())\n",
    "\n",
    "# Histogram\n",
    "n, b, patches = ax.hist(\n",
    "    vals,\n",
    "    bins=bins,\n",
    "    rwidth=0.92,\n",
    "    edgecolor=\"white\",\n",
    "    linewidth=0.8,\n",
    "    alpha=0.95\n",
    ")\n",
    "\n",
    "# Color each bar by its bin center\n",
    "for i, p in enumerate(patches):\n",
    "    center = 0.5 * (b[i] + b[i+1])\n",
    "    p.set_facecolor(cmap(norm(center)))\n",
    "\n",
    "# Colorbar keyed to Average SITR values\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cb = plt.colorbar(sm, ax=ax, pad=0.015)\n",
    "cb.set_label(\"Average SITR (bin color)\")\n",
    "\n",
    "# Mean and median markers\n",
    "mu = float(np.mean(vals))\n",
    "med = float(np.median(vals))\n",
    "ax.axvline(mu, linestyle=\"--\", linewidth=1.5, label=f\"Mean = {mu:.2f}\")\n",
    "ax.axvline(med, linestyle=\"-.\", linewidth=1.5, label=f\"Median = {med:.2f}\")\n",
    "\n",
    "# Labels/title\n",
    "ax.set_xlabel(\"Average SITR\")\n",
    "ax.set_ylabel(\"Number of Players\")\n",
    "ax.set_title(\"Distribution of SITR for Defensive Backs\")\n",
    "\n",
    "# Light grid + tidy spines\n",
    "ax.grid(True, alpha=0.18)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_alpha(0.5)\n",
    "\n",
    "ax.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# --- values ---\n",
    "db_vals = db_df[\"Average SITR\"].astype(float).to_numpy()\n",
    "lb_vals = lb_df[\"Average SITR\"].astype(float).to_numpy()\n",
    "\n",
    "# --- shared bins ---\n",
    "bins = 24\n",
    "xmin = min(db_vals.min(), lb_vals.min())\n",
    "xmax = max(db_vals.max(), lb_vals.max())\n",
    "edges = np.linspace(xmin, xmax, bins + 1)\n",
    "centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "bar_h = np.diff(edges)\n",
    "\n",
    "# compare shapes; set to False for counts\n",
    "density = True\n",
    "h_db, _ = np.histogram(db_vals, bins=edges, density=density)\n",
    "h_lb, _ = np.histogram(lb_vals, bins=edges, density=density)\n",
    "\n",
    "# --- single colormap used for BOTH sides (same pattern) ---\n",
    "cmap = plt.get_cmap(\"turbo\") if \"turbo\" in plt.colormaps() else plt.get_cmap(\"plasma\")\n",
    "norm = colors.Normalize(vmin=centers.min(), vmax=centers.max())\n",
    "colors_per_bin = [cmap(norm(c)) for c in centers]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 7))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Bars: DB to the right, LB mirrored to the left\n",
    "bars_db = ax.barh(centers,  h_db, height=bar_h, align=\"center\",\n",
    "                  edgecolor=\"white\", linewidth=0.9, alpha=0.95, zorder=3)\n",
    "bars_lb = ax.barh(centers, -h_lb, height=bar_h, align=\"center\",\n",
    "                  edgecolor=\"white\", linewidth=0.9, alpha=0.95, zorder=3)\n",
    "\n",
    "# Apply identical color pattern on both sides\n",
    "for rect, col in zip(bars_db, colors_per_bin):\n",
    "    rect.set_facecolor(col)\n",
    "for rect, col in zip(bars_lb, colors_per_bin):\n",
    "    rect.set_facecolor(col)\n",
    "\n",
    "# Symmetric x-axis\n",
    "m = max(h_db.max(), h_lb.max())\n",
    "ax.set_xlim(-m * 1.2, m * 1.2)\n",
    "ax.set_ylim(xmin, xmax)\n",
    "\n",
    "# Bold center divider\n",
    "ax.axvline(0, color=\"black\", linewidth=5, alpha=0.95, zorder=5)\n",
    "\n",
    "# Colored medians (red = DB, blue = LB)\n",
    "med_db = float(np.median(db_vals))\n",
    "med_lb = float(np.median(lb_vals))\n",
    "ax.axhline(med_db, linestyle=\"--\", linewidth=3.2, color=\"red\",  label=f\"DB median {med_db:.2f}\",  zorder=4)\n",
    "ax.axhline(med_lb, linestyle=\"-.\", linewidth=3.2, color=\"blue\", label=f\"LB median {med_lb:.2f}\", zorder=4)\n",
    "\n",
    "# Huge side labels centered in each half (behind bars so they don't cover data)\n",
    "ax.text(0.25, 0.50, \"LB\", color=\"blue\", fontsize=48, fontweight=\"bold\",\n",
    "        ha=\"center\", va=\"center\", transform=ax.transAxes, alpha=0.16, zorder=1)\n",
    "ax.text(0.75, 0.50, \"DB\", color=\"red\",  fontsize=48, fontweight=\"bold\",\n",
    "        ha=\"center\", va=\"center\", transform=ax.transAxes, alpha=0.16, zorder=1)\n",
    "\n",
    "# Bigger title/labels/ticks\n",
    "ax.set_title(\"Solo Tackle Impact Rating Distribution— Linebackers (left) vs Defensive Backs (right)\", fontsize=20, pad=12)\n",
    "ax.set_xlabel(\"Density\" if density else \"Players\", fontsize=18)\n",
    "ax.set_ylabel(\"Average SITR\", fontsize=18)\n",
    "ax.tick_params(axis=\"both\", labelsize=13)\n",
    "\n",
    "# Pretty x-ticks: show absolute values\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(\n",
    "    lambda v, pos: f\"{abs(v):.2f}\" if density else f\"{int(abs(v))}\"\n",
    "))\n",
    "\n",
    "ax.grid(True, axis=\"x\", alpha=0.25)\n",
    "ax.legend(frameon=False, loc=\"upper right\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"stir_db_vs_lb.png\", dpi=300, bbox_inches=\"tight\")  # high-res PNG\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined3[combined3[\"position\"]==\"DB\"].sort_values(by=\"Average SITR\",ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
